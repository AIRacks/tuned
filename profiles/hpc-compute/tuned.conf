#
# tuned configuration
#

[main]
summary=Optimize for HPC compute workloads
include=throughput-performance

[sysctl]
# Busy polling helps reduce latency in the network receive path
# by allowing socket layer code to poll the receive queue of a
# network device, and disabling network interrupts.
# busy_read value greater than 0 enables busy polling. Recommended
# net.core.busy_read value is 50.
# busy_poll value greater than 0 enables polling globally. 
# Recommended net.core.busy_poll value is 50 
net.core.busy_read=50
net.core.busy_poll=50

# TCP fast open reduces network latency by enabling data exchange
# during the sender's initial TCP SYN. The value 3 enables fast open
# on client and server connections.
net.ipv4.tcp_fastopen=3

# Most HPC applications have NUMA knowledge. Disabling automatic NUMA
# balancing prevents unwanted memory unmapping. 
kernel.numa_balancing=0

# The total time the scheduler will consider a migrated process
# "cache hot" and thus less likely to be re-migrated
# (system default is 500000, i.e. 0.5 ms)
kernel.sched_migration_cost_ns=5000000

[vm]
# Most HPC applications can take advantage of large page sizes.
# This setting ensures that THP is forced on.
transparent_hugepages=always